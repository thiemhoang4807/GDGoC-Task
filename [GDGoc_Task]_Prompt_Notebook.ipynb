{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **C√¢u 1**\n",
        "\n",
        "# **ƒê·ªÄ B√ÄI**\n",
        "1. Ph√¢n bi·ªát gi·ªØa Zero-shot, One-shot v√† Few-shot Prompting. Khi n√†o n√™n d√πng t·ª´ng k·ªπ thu·∫≠t?\n",
        "2. H√£y m√¥ t·∫£ chi ti·∫øt v·ªÅ Chain-of-thought Prompting.\n",
        "3. System Prompt l√† g√¨ v√† n√≥ kh√°c g√¨ so v·ªõi User Prompt v√† Assistant Prompt trong ki·∫øn tr√∫c ho·∫°t ƒë·ªông c·ªßa LLM?\n",
        "4. H√£y ph√¢n t√≠ch m·ªôt v√≠ d·ª• System Prompt t·ªët v√† m·ªôt v√≠ d·ª• System Prompt k√©m, gi·∫£i th√≠ch l√Ω do v√† s·ª± kh√°c bi·ªát v·ªÅ ch·∫•t l∆∞·ª£ng ƒë·∫ßu ra c·ªßa m√¥ h√¨nh.\n",
        "5. C√°c tham s·ªë khi g·ªçi m√¥ h√¨nh ng√¥n ng·ªØ (nh∆∞ Temperature, Max length, Context Window,‚Ä¶) ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn k·∫øt qu·∫£ sinh vƒÉn b·∫£n?"
      ],
      "metadata": {
        "id": "KwDaAdAkQchy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C√¢u 1.1**\n",
        "### **a. Zero-shot Prompting**\n",
        "**ƒê·ªãnh nghƒ©a:** B·∫°n ƒë∆∞a ra y√™u c·∫ßu tr·ª±c ti·∫øp cho m√¥ h√¨nh m√† kh√¥ng cung c·∫•p b·∫•t k·ª≥ v√≠ d·ª• minh h·ªça n√†o v·ªÅ c√°ch gi·∫£i quy·∫øt hay ƒë·ªãnh d·∫°ng ƒë·∫ßu ra. M√¥ h√¨nh ho√†n to√†n d·ª±a v√†o ki·∫øn th·ª©c ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥.\n",
        "\n",
        "Khi n√†o n√™n d√πng:\n",
        "Khi nhi·ªám v·ª• ƒë∆°n gi·∫£n, ph·ªï bi·∫øn (d·ªãch thu·∫≠t, t√≥m t·∫Øt vƒÉn b·∫£n, tr·∫£ l·ªùi ki·∫øn th·ª©c chung).\n",
        "\n",
        "### **b. One-shot Prompting**\n",
        "**ƒê·ªãnh nghƒ©a:** B·∫°n cung c·∫•p m·ªôt v√≠ d·ª• duy nh·∫•t (bao g·ªìm c·∫£ ƒë·∫ßu v√†o v√† ƒë·∫ßu ra mong mu·ªën) tr∆∞·ªõc khi ƒë∆∞a ra c√¢u h·ªèi th·ª±c t·∫ø.\n",
        "\n",
        "Khi n√†o n√™n d√πng:\n",
        "- Khi b·∫°n c·∫ßn m√¥ h√¨nh tu√¢n theo m·ªôt ƒë·ªãnh d·∫°ng ho·∫∑c gi·ªçng vƒÉn c·ª• th·ªÉ.\n",
        "- Khi nhi·ªám v·ª• h∆°i l·∫° nh∆∞ng m·ªôt v√≠ d·ª• l√† ƒë·ªß ƒë·ªÉ m√¥ h√¨nh hi·ªÉu quy lu·∫≠t.\n",
        "\n",
        "### **c. Few-shot Prompting**\n",
        "**ƒê·ªãnh nghƒ©a:** B·∫°n cung c·∫•p t·ª´ 2 ƒë·∫øn 5 (ho·∫∑c nhi·ªÅu h∆°n) v√≠ d·ª• ho√†n ch·ªânh ƒë·ªÉ d·∫°y m√¥ h√¨nh v·ªÅ quy lu·∫≠t ph·ª©c t·∫°p.\n",
        "Khi n√†o n√™n d√πng:\n",
        "- Khi nhi·ªám v·ª• ph·ª©c t·∫°p, ƒë√≤i h·ªèi t∆∞ duy logic cao ho·∫∑c tu√¢n theo quy t·∫Øc kh√≥ di·ªÖn gi·∫£i b·∫±ng l·ªùi.\n",
        "- Khi Zero-shot v√† One-shot th·∫•t b·∫°i ho·∫∑c tr·∫£ v·ªÅ k·∫øt qu·∫£ kh√¥ng nh·∫•t qu√°n.\n",
        "- S·ª≠ d·ª•ng cho c√°c t√°c v·ª• ph√¢n lo·∫°i c·∫£m x√∫c kh√≥, tr√≠ch xu·∫•t d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·∫∑c bi·ªát."
      ],
      "metadata": {
        "id": "rC5_6uFlR0eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C√¢u 1.2**\n",
        "Chain of thought l√† kƒ© thu·∫≠t khi·∫øn LLM kh√¥ng tr·∫£ l·ªùi ngay l·∫≠p t·ª©c m√† ph·∫£i suy nghƒ© t·ª´ng b∆∞·ªõc, v√† gi·∫£i th√≠ch c√°c suy lu·∫≠n c·ªßa m√¨nh. LLM s·∫Ω chia nh·ªè c√°c b∆∞·ªõc suy nghƒ© c·ªßa m√¨nh, gi·∫£i quy·∫øt t·ª´ng ph·∫ßn v√† ƒëi ƒë·∫øn k·∫øt qu·∫£ cu·ªëi c√πng.\n",
        "\n",
        "V√≠ d·ª•:\n",
        "- Prompt th√¥ng th∆∞·ªùng: \"Nh√≥m c√≥ 5 qu·∫£ t√°o, ƒÉn 2 qu·∫£, mua th√™m 3 qu·∫£. C√≤n m·∫•y qu·∫£?\" -> AI tr·∫£ l·ªùi ngay: \"6 qu·∫£\". Nh∆∞ng c√°ch n√†y d·ªÖ sai ·ªü c√°c b√†i to√°n ph·ª©c t·∫°p.\n",
        "- Chain of thought Prompting: AI tr·∫£ l·ªùi: \"Ban ƒë·∫ßu c√≥ 5. ƒÇn 2 c√≤n 3. Mua th√™m 3 th√¨ l·∫•y 3 + 3 = 6. V·∫≠y k·∫øt qu·∫£ l√† 6.\"\n",
        "\n",
        "**T√°c d·ª•ng:** C·∫£i thi·ªán ƒë√°ng k·ªÉ kh·∫£ nƒÉng gi·∫£i to√°n, gi·∫£i ƒë·ªë logic v√† suy lu·∫≠n.\n"
      ],
      "metadata": {
        "id": "MCpoS-RTUjw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C√¢u 1.3**\n",
        "**System Prompt:** Thi·∫øt l·∫≠p h√†nh vi, t√≠nh c√°ch, b·ªëi c·∫£nh v√† c√°c gi·ªõi h·∫°n cho m√¥ h√¨nh, l√† ch·ªâ th·ªã cao nh·∫•t, th∆∞·ªùng ·∫©n ƒëi ƒë·ªëi v·ªõi ng∆∞·ªùi d√πng.\n",
        "\n",
        "**User Prompt:** Input t·ª´ ng∆∞·ªùi d√πng.\n",
        "\n",
        "**Assistant Prompt:** L√† c√¢u tr·∫£ l·ªùi do AI sinh ra."
      ],
      "metadata": {
        "id": "ntMf0RpoWEDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**C√¢u 1.4**\n",
        "**System Prompt k√©m:** B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n h·ªØu √≠ch.\n",
        "\n",
        "L√Ω do k√©m:\n",
        "- Qu√° chung chung, kh√¥ng x√°c ƒë·ªãnh r√µ ph·∫°m vi ki·∫øn th·ª©c.\n",
        "- Kh√¥ng quy ƒë·ªãnh v·ªÅ ƒë·∫ßu ra (d√†i hay ng·∫Øn).\n",
        "\n",
        "-> K·∫øt qu·∫£ ƒë·∫ßu ra th∆∞·ªùng lan man, thi·∫øu tr·ªçng t√¢m, ƒë√¥i khi s·∫Ω kh√¥ng ch√≠nh x√°c.\n",
        "\n",
        "\n",
        "\n",
        "**System Prompt t·ªët:** B·∫°n l√† m·ªôt gi·∫£ng vi√™n trong lƒ©nh v·ª±c gi·∫£i t√≠ch, ƒë·∫∑c bi·ªát l√† vi t√≠ch ph√¢n. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p t√¥i gi·∫£i quy·∫øt b√†i to√°n sau v√† gi·∫£i th√≠ch m·ªôt c√°ch d·ªÖ hi·ªÉu nh·∫•t.\n",
        "\n",
        "L√Ω do t·ªët:\n",
        "- ƒê·ªãnh danh r√µ r√†ng (gi·∫£ng vi√™n trong lƒ©nh v·ª±c gi·∫£i t√≠ch).\n",
        "- Ph·∫°m vi r√µ r√†ng, nhi·ªám v·ª• c·ª• th·ªÉ.\n",
        "\n",
        "-> K·∫øt qu·∫£ ƒë·∫ßu ra s·∫Ω s√°t v·ªõi th·ª© ta c·∫ßn, t·∫°o c·∫£m gi√°c chuy√™n nghi·ªáp v√† tin c·∫≠y h∆°n."
      ],
      "metadata": {
        "id": "MAlirJZUY-Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**C√¢u 1.5**\n",
        "1. Temperature (Nhi·ªát ƒë·ªô):\n",
        "- Ki·ªÉm so√°t ƒë·ªô ng·∫´u nhi√™n v√† s√°ng t·∫°o.\n",
        "- T·ª´ 0 - 0.3 (th·∫•p): M√¥ h√¨nh ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t. K·∫øt qu·∫£ nh·∫•t qu√°n, logic, ch√≠nh x√°c (d√πng cho to√°n, code, tr√≠ch xu·∫•t d·ªØ li·ªáu).\n",
        "- T·ª´ 0.7 - 1+ (cao): M√¥ h√¨nh d√°m ch·ªçn c√°c t·ª´ √≠t ph·ªï bi·∫øn h∆°n. K·∫øt qu·∫£ s√°ng t·∫°o, ƒëa d·∫°ng, b·∫•t ng·ªù (d√πng cho l√†m th∆°, vi·∫øt truy·ªán).\n",
        "\n",
        "2. Max Length\n",
        "- Gi·ªõi h·∫°n s·ªë t·ª´ m√† m√¥ h√¨nh ƒë∆∞·ª£c ph√©p sinh ra.\n",
        "- N·∫øu ƒë·∫∑t qu√° th·∫•p, c√¢u tr·∫£ l·ªùi s·∫Ω b·ªã c·∫Øt ngang gi·ªØa ch·ª´ng. N·∫øu ƒë·∫∑t qu√° cao c√≥ th·ªÉ t·ªën chi ph√≠ v√† th·ªùi gian ch·ªù.\n",
        "\n",
        "3. Context window\n",
        "- L√† b·ªô nh·ªõ ng·∫Øn h·∫°n c·ªßa m√¥ h√¨nh.\n",
        "- N·∫øu ƒëo·∫°n h·ªôi tho·∫°i v∆∞·ª£t qu√° context window c·ªßa m√¥ h√¨nh th√¨ m√¥ h√¨nh s·∫Ω \"qu√™n\" nh·ªØng th√¥ng tin ban ƒë·∫ßu."
      ],
      "metadata": {
        "id": "uPw2cZqqcmCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C√¢u 2**"
      ],
      "metadata": {
        "id": "xyLbJZwbdPAq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "## C√†i th∆∞ vi·ªán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai requests langchain-groq groq google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "Groq API Key\n",
        "\n",
        "ƒê·ªÉ s·ª≠ d·ª•ng Groq API, b·∫°n c·∫ßn m·ªôt API key. Truy c·∫≠p https://groq.com/\n",
        ", ƒëƒÉng k√Ω t√†i kho·∫£n ho·∫∑c ƒëƒÉng nh·∫≠p, sau ƒë√≥ v√†o m·ª•c Dashboard ‚Üí API Keys. T·∫°i ƒë√¢y, b·∫°n c√≥ th·ªÉ t·∫°o m·ªôt API key m·ªõi v√† s·ª≠ d·ª•ng n√≥ trong notebook ƒë·ªÉ g·ªçi Groq LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# ======== Nh·∫≠p Groq API Key ========\n",
        "GROQ_API_KEY = \"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\"  # @param {type: \"string\", placeholder: \"[your-groq-api-key]\"}\n",
        "if not GROQ_API_KEY or GROQ_API_KEY == \"[your-groq-api-key]\":\n",
        "    GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QqRWdPGmW3NJ"
      },
      "outputs": [],
      "source": [
        "import google.genai as genai\n",
        "\n",
        "PROJECT_ID = \"[GDGoC Task] Prompt Engineering\"\n",
        "LOCATION = \"asia-east1-a\"\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFPpCRtXRl4"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IQYu_9SvXQah"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"llama-3.1-8b-instant\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVOtUNJ5X0PY"
      },
      "source": [
        "## Prompt engineering best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_e0fEPX60q"
      },
      "source": [
        "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
        "\n",
        "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
        "\n",
        "In this section, you'll cover the following best practices when engineering prompts:\n",
        "\n",
        "* Be concise\n",
        "* Be specific, and well-defined\n",
        "* Ask one task at a time\n",
        "* Improve response quality by including examples\n",
        "* Turn generative tasks to classification tasks to improve safety"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup ChatGroq"
      ],
      "metadata": {
        "id": "VRk6liQ0WAAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=MODEL_ID,\n",
        "    temperature= 0,\n",
        "    max_tokens= None,\n",
        "    #reasoning_format=\"raw\"\n",
        ")"
      ],
      "metadata": {
        "id": "LRdeUJuCV40W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pY4XX0OX9_Y"
      },
      "source": [
        "### Be concise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlRpxyxGYA1K"
      },
      "source": [
        "üõë Not recommended. The prompt below is unnecessarily verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YKV4G-CfXdbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "be351e5f-567f-4cc5-87ff-4035161e58d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë g·ª£i √Ω v·ªÅ c√°i t√™n hay cho c·ª≠a h√†ng hoa kh√¥:\n\n1. **Hoa Kh√¥ S·∫Øc M·ªõi**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± t∆∞∆°i m·ªõi v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n2. **C√°nh Hoa Kh√¥**: T√™n n√†y ƒë∆°n gi·∫£n v√† d·ªÖ nh·ªõ, ƒë·ªìng th·ªùi c≈©ng g·ª£i l√™n h√¨nh ·∫£nh c·ªßa hoa kh√¥.\n3. **Hoa Kh√¥ T√¨nh Y√™u**: T√™n n√†y ph√π h·ª£p v·ªõi c·ª≠a h√†ng hoa kh√¥ chuy√™n b√°n hoa cho c√°c d·ªãp l·ªÖ h·ªôi, ƒë·∫∑c bi·ªát l√† d·ªãp Valentine.\n4. **C·ª≠a H√†ng Hoa Kh√¥ S·∫Øc ƒê·∫πp**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± ƒë·∫πp m·∫Øt v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n5. **Hoa Kh√¥ S·∫Øc M·ªôc**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± t·ª± nhi√™n v√† m·ªôc m·∫°c c·ªßa hoa kh√¥.\n6. **C√°nh Hoa Kh√¥ T√¨nh C·∫£m**: T√™n n√†y ph√π h·ª£p v·ªõi c·ª≠a h√†ng hoa kh√¥ chuy√™n b√°n hoa cho c√°c d·ªãp l·ªÖ h·ªôi, ƒë·∫∑c bi·ªát l√† d·ªãp t√¨nh y√™u.\n7. **Hoa Kh√¥ S·∫Øc ƒê·ªôc**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± ƒë·ªôc ƒë√°o v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n8. **C·ª≠a H√†ng Hoa Kh√¥ S·∫Øc T√¨nh**: T√™n n√†y ph√π h·ª£p v·ªõi c·ª≠a h√†ng hoa kh√¥ chuy√™n b√°n hoa cho c√°c d·ªãp l·ªÖ h·ªôi, ƒë·∫∑c bi·ªát l√† d·ªãp t√¨nh y√™u.\n9. **Hoa Kh√¥ S·∫Øc M·ªõi M·ªôc**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± t∆∞∆°i m·ªõi v√† t·ª± nhi√™n c·ªßa hoa kh√¥.\n10. **C√°nh Hoa Kh√¥ S·∫Øc ƒê·∫πp M·ªôc**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± ƒë·∫πp m·∫Øt v√† t·ª± nhi√™n c·ªßa hoa kh√¥.\n\nHy v·ªçng nh·ªØng g·ª£i √Ω n√†y s·∫Ω gi√∫p b·∫°n t√¨m ƒë∆∞·ª£c c√°i t√™n ph√π h·ª£p cho c·ª≠a h√†ng hoa kh√¥ c·ªßa m√¨nh!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"B·∫°n nghƒ© ƒë√¢u l√† nh·ªØng c√°i t√™n hay cho c·ª≠a h√†ng hoa chuy√™n b√°n hoa kh√¥?.\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrJexRHJYnmC"
      },
      "source": [
        "‚úÖ Recommended. The prompt below is to the point and concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHetn9lCYrXB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "239f8535-686e-409d-ff98-175b564b9c42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë g·ª£i √Ω t√™n c·ª≠a h√†ng hoa kh√¥:\n\n1. **Hoa Kh√¥ S·∫Øc M·ªõi**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± t∆∞∆°i m·ªõi v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n2. **C√°nh Hoa Kh√¥**: T√™n n√†y ƒë∆°n gi·∫£n v√† d·ªÖ nh·ªõ, ƒë·ªìng th·ªùi c≈©ng g·ª£i l√™n h√¨nh ·∫£nh c·ªßa hoa kh√¥.\n3. **Hoa Kh√¥ T√¨nh Y√™u**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ t√¨nh y√™u v√† s·ª± ·∫•m √°p c·ªßa hoa kh√¥.\n4. **C·ª≠a H√†ng Hoa Kh√¥**: T√™n n√†y ƒë∆°n gi·∫£n v√† d·ªÖ hi·ªÉu, ƒë·ªìng th·ªùi c≈©ng g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ c·ª≠a h√†ng chuy√™n b√°n hoa kh√¥.\n5. **Hoa Kh√¥ S·∫Øc ƒê·∫πp**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± ƒë·∫πp ƒë·∫Ω v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n6. **C√°nh Hoa S·∫Øc M·ªõi**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ s·ª± t∆∞∆°i m·ªõi v√† ƒë·ªôc ƒë√°o c·ªßa hoa kh√¥.\n7. **Hoa Kh√¥ T√¨nh C·∫£m**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ t√¨nh c·∫£m v√† s·ª± ·∫•m √°p c·ªßa hoa kh√¥.\n8. **C·ª≠a H√†ng Hoa Kh√¥ S·∫Øc ƒê·∫πp**: T√™n n√†y g·ª£i l√™n √Ω t∆∞·ªüng v·ªÅ c·ª≠a h√†ng chuy√™n b√°n hoa kh√¥ v√† s·ª± ƒë·∫πp ƒë·∫Ω c·ªßa hoa kh√¥.\n\nHy v·ªçng nh·ªØng g·ª£i √Ω n√†y s·∫Ω gi√∫p b·∫°n t√¨m ƒë∆∞·ª£c t√™n c·ª≠a h√†ng hoa kh√¥ ph√π h·ª£p!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"H√£y g·ª£i √Ω m·ªôt c√°i t√™n hay cho c·ª≠a h√†ng hoa chuy√™n b√°n hoa kh√¥ nhi·ªÅu h∆°n hoa t∆∞∆°i.\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTAvdOHY0OC"
      },
      "source": [
        "### Be specific, and well-defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTH4GEIgY1dp"
      },
      "source": [
        "Suppose that you want to brainstorm creative ways to describe Earth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5BmXBiGY4KC"
      },
      "source": [
        "üõë The prompt below might be a bit too generic (which is certainly OK if you'd like to ask a generic question!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eHBaMvv7Y6mR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "479781f2-d0ad-4b0c-b493-b50b628af91f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** GDGoC-UIT (Gia ƒê·ªãnh Go C√π - ƒê·∫°i h·ªçc Qu·ªëc gia Th√†nh ph·ªë H·ªì Ch√≠ Minh) l√† m·ªôt trong nh·ªØng t·ªï ch·ª©c nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá h√†ng ƒë·∫ßu t·∫°i Vi·ªát Nam. GDGoC-UIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 1997 t·∫°i ƒê·∫°i h·ªçc Qu·ªëc gia Th√†nh ph·ªë H·ªì Ch√≠ Minh (ƒêHQG TP.HCM), v·ªõi m·ª•c ti√™u nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá ƒë·ªÉ ph·ª•c v·ª• s·ª± ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi c·ªßa Vi·ªát Nam.\n\nGDGoC-UIT c√≥ tr·ª• s·ªü t·∫°i Th√†nh ph·ªë H·ªì Ch√≠ Minh v√† c√≥ m·∫°ng l∆∞·ªõi nghi√™n c·ª©u r·ªông kh·∫Øp c·∫£ n∆∞·ªõc. T·ªï ch·ª©c n√†y t·∫≠p trung v√†o c√°c lƒ©nh v·ª±c nghi√™n c·ª©u nh∆∞:\n\n- C√¥ng ngh·ªá th√¥ng tin v√† truy·ªÅn th√¥ng\n- C√¥ng ngh·ªá sinh h·ªçc v√† y sinh\n- C√¥ng ngh·ªá v·∫≠t li·ªáu v√† nƒÉng l∆∞·ª£ng\n- C√¥ng ngh·ªá m√¥i tr∆∞·ªùng v√† b·∫£o v·ªá t√†i nguy√™n\n- C√¥ng ngh·ªá x√£ h·ªôi v√† ph√°t tri·ªÉn c·ªông ƒë·ªìng\n\nGDGoC-UIT ƒë√£ th·ª±c hi·ªán nhi·ªÅu d·ª± √°n nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá quan tr·ªçng, bao g·ªìm:\n\n- Ph√°t tri·ªÉn h·ªá th·ªëng qu·∫£n l√Ω d·ªØ li·ªáu v√† th√¥ng tin cho c√°c c∆° quan nh√† n∆∞·ªõc\n- Ph√°t tri·ªÉn c√¥ng ngh·ªá sinh h·ªçc v√† y sinh ƒë·ªÉ s·∫£n xu·∫•t c√°c s·∫£n ph·∫©m y t·∫ø v√† th·ª±c ph·∫©m\n- Ph√°t tri·ªÉn c√¥ng ngh·ªá v·∫≠t li·ªáu v√† nƒÉng l∆∞·ª£ng ƒë·ªÉ s·∫£n xu·∫•t c√°c s·∫£n ph·∫©m nƒÉng l∆∞·ª£ng s·∫°ch\n- Ph√°t tri·ªÉn c√¥ng ngh·ªá m√¥i tr∆∞·ªùng v√† b·∫£o v·ªá t√†i nguy√™n ƒë·ªÉ b·∫£o v·ªá m√¥i tr∆∞·ªùng v√† t√†i nguy√™n thi√™n nhi√™n\n- Ph√°t tri·ªÉn c√¥ng ngh·ªá x√£ h·ªôi v√† ph√°t tri·ªÉn c·ªông ƒë·ªìng ƒë·ªÉ h·ªó tr·ª£ s·ª± ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi c·ªßa c√°c c·ªông ƒë·ªìng ƒë·ªãa ph∆∞∆°ng\n\nGDGoC-UIT c≈©ng c√≥ m·ªëi quan h·ªá h·ª£p t√°c v·ªõi nhi·ªÅu t·ªï ch·ª©c nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá qu·ªëc t·∫ø, bao g·ªìm c√°c t·ªï ch·ª©c nh∆∞ UNESCO, WHO, UNDP, v√† c√°c t·ªï ch·ª©c nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá h√†ng ƒë·∫ßu tr√™n th·∫ø gi·ªõi.\n\nT√≥m l·∫°i, GDGoC-UIT l√† m·ªôt t·ªï ch·ª©c nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá h√†ng ƒë·∫ßu t·∫°i Vi·ªát Nam, v·ªõi m·ª•c ti√™u nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá ƒë·ªÉ ph·ª•c v·ª• s·ª± ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi c·ªßa Vi·ªát Nam."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"H√£y k·ªÉ cho t√¥i nghe v·ªÅ GDGoC-UIT.\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iyvEbteZnFL"
      },
      "source": [
        "‚úÖ Recommended. The prompt below is specific and well-defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JQ80z8urZnne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "206f056c-8f1d-403b-ade3-7dcbd571b12d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** GDGoC-UIT (Gi√°o d·ª•c ƒê·∫°i h·ªçc Qu·ªëc gia C√¥ng ngh·ªá - ƒê·∫°i h·ªçc Qu·ªëc gia Th√†nh ph·ªë H·ªì Ch√≠ Minh) l√† m·ªôt trong nh·ªØng tr∆∞·ªùng ƒë·∫°i h·ªçc h√†ng ƒë·∫ßu t·∫°i Vi·ªát Nam. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë ƒëi·ªÉm khi·∫øn GDGoC-UIT kh√°c bi·ªát so v·ªõi c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc kh√°c:\n\n1. **M√¥ h√¨nh ƒë√†o t·∫°o qu·ªëc t·∫ø**: GDGoC-UIT ƒë∆∞·ª£c th√†nh l·∫≠p tr√™n c∆° s·ªü h·ª£p t√°c gi·ªØa Vi·ªát Nam v√† c√°c n∆∞·ªõc ph√°t tri·ªÉn, mang l·∫°i cho sinh vi√™n c∆° h·ªôi h·ªçc t·∫≠p v√† nghi√™n c·ª©u trong m√¥i tr∆∞·ªùng qu·ªëc t·∫ø.\n\n2. **Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ƒëa d·∫°ng**: Tr∆∞·ªùng cung c·∫•p c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ƒëa d·∫°ng, t·ª´ c√°c ng√†nh khoa h·ªçc t·ª± nhi√™n, k·ªπ thu·∫≠t, c√¥ng ngh·ªá, kinh t·∫ø, x√£ h·ªôi, ƒë·∫øn c√°c ng√†nh ngh·ªá thu·∫≠t v√† thi·∫øt k·∫ø.\n\n3. **S·ª± k·∫øt h·ª£p gi·ªØa l√Ω thuy·∫øt v√† th·ª±c h√†nh**: GDGoC-UIT nh·∫•n m·∫°nh s·ª± k·∫øt h·ª£p gi·ªØa l√Ω thuy·∫øt v√† th·ª±c h√†nh, gi√∫p sinh vi√™n c√≥ th·ªÉ √°p d·ª•ng ki·∫øn th·ª©c v√†o th·ª±c t·∫ø v√† ph√°t tri·ªÉn k·ªπ nƒÉng m·ªÅm.\n\n4. **M√¥i tr∆∞·ªùng h·ªçc t·∫≠p hi·ªán ƒë·∫°i**: Tr∆∞·ªùng s·ªü h·ªØu c√°c c∆° s·ªü v·∫≠t ch·∫•t hi·ªán ƒë·∫°i, bao g·ªìm c√°c ph√≤ng th√≠ nghi·ªám, th∆∞ vi·ªán, ph√≤ng h·ªçc, v√† c√°c c√¥ng ngh·ªá th√¥ng tin ti√™n ti·∫øn.\n\n5. **S·ª± tham gia c·ªßa c√°c chuy√™n gia qu·ªëc t·∫ø**: GDGoC-UIT c√≥ s·ª± tham gia c·ªßa c√°c chuy√™n gia qu·ªëc t·∫ø trong qu√° tr√¨nh gi·∫£ng d·∫°y v√† nghi√™n c·ª©u, gi√∫p sinh vi√™n c√≥ th·ªÉ ti·∫øp c·∫≠n v·ªõi ki·∫øn th·ª©c v√† k·ªπ nƒÉng hi·ªán ƒë·∫°i.\n\n6. **Ch∆∞∆°ng tr√¨nh nghi√™n c·ª©u v√† ph√°t tri·ªÉn**: Tr∆∞·ªùng c√≥ c√°c ch∆∞∆°ng tr√¨nh nghi√™n c·ª©u v√† ph√°t tri·ªÉn m·∫°nh m·∫Ω, gi√∫p sinh vi√™n c√≥ th·ªÉ tham gia v√†o c√°c d·ª± √°n nghi√™n c·ª©u v√† ph√°t tri·ªÉn c√¥ng ngh·ªá.\n\n7. **S·ª± k·∫øt n·ªëi v·ªõi c√°c doanh nghi·ªáp v√† t·ªï ch·ª©c**: GDGoC-UIT c√≥ s·ª± k·∫øt n·ªëi v·ªõi c√°c doanh nghi·ªáp v√† t·ªï ch·ª©c qu·ªëc t·∫ø, gi√∫p sinh vi√™n c√≥ th·ªÉ c√≥ c∆° h·ªôi th·ª±c t·∫≠p, l√†m vi·ªác v√† ph√°t tri·ªÉn s·ª± nghi·ªáp.\n\n8. **Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o li√™n k·∫øt qu·ªëc t·∫ø**: Tr∆∞·ªùng cung c·∫•p c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o li√™n k·∫øt qu·ªëc t·∫ø v·ªõi c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc h√†ng ƒë·∫ßu tr√™n th·∫ø gi·ªõi, gi√∫p sinh vi√™n c√≥ th·ªÉ h·ªçc t·∫≠p v√† nghi√™n c·ª©u trong m√¥i tr∆∞·ªùng qu·ªëc t·∫ø.\n\n9. **S·ª± ƒëa d·∫°ng v·ªÅ ngu·ªìn l·ª±c**: GDGoC-UIT c√≥ s·ª± ƒëa d·∫°ng v·ªÅ ngu·ªìn l·ª±c, bao g·ªìm c√°c ngu·ªìn l·ª±c t√†i ch√≠nh, nh√¢n l·ª±c, v√† c√¥ng ngh·ªá, gi√∫p tr∆∞·ªùng c√≥ th·ªÉ cung c·∫•p c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ch·∫•t l∆∞·ª£ng cao.\n\n10. **S·ª± cam k·∫øt v·ªõi s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng**: Tr∆∞·ªùng cam k·∫øt v·ªõi s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng, gi√∫p sinh vi√™n c√≥ th·ªÉ ph√°t tri·ªÉn s·ª± nghi·ªáp v√† ƒë√≥ng g√≥p v√†o s·ª± ph√°t tri·ªÉn c·ªßa x√£ h·ªôi.\n\nT√≥m l·∫°i, GDGoC-UIT l√† m·ªôt trong nh·ªØng tr∆∞·ªùng ƒë·∫°i h·ªçc h√†ng ƒë·∫ßu t·∫°i Vi·ªát Nam, v·ªõi c√°c ƒëi·ªÉm kh√°c bi·ªát v·ªÅ m√¥ h√¨nh ƒë√†o t·∫°o qu·ªëc t·∫ø, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ƒëa d·∫°ng, s·ª± k·∫øt h·ª£p gi·ªØa l√Ω thuy·∫øt v√† th·ª±c h√†nh, m√¥i tr∆∞·ªùng h·ªçc t·∫≠p hi·ªán ƒë·∫°i, s·ª± tham gia c·ªßa c√°c chuy√™n gia qu·ªëc t·∫ø, ch∆∞∆°ng tr√¨nh nghi√™n c·ª©u v√† ph√°t tri·ªÉn, s·ª± k·∫øt n·ªëi v·ªõi c√°c doanh nghi·ªáp v√† t·ªï ch·ª©c, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o li√™n k·∫øt qu·ªëc t·∫ø, s·ª± ƒëa d·∫°ng v·ªÅ ngu·ªìn l·ª±c, v√† s·ª± cam k·∫øt v·ªõi s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"Li·ªát k√™ nh·ªØng ƒëi·ªÉm khi·∫øn GDGoC-UIT kh√°c bi·ªát so v·ªõi c√°c c√¢u l·∫≠p b·ªô kh√°c.\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5kmfZYHZsJ7"
      },
      "source": [
        "### Ask one task at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsAezxeYZuUN"
      },
      "source": [
        "üõë Not recommended. The prompt below has two parts to the question that could be asked separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ElywPXpuZtWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "6e528504-249c-4cd7-cd12-7eae31a70405"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** **Ph∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc**\n\nPh∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc l√† s·ª≠ d·ª•ng b·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas v·ªõi m·ªôt n·ªìi th·ªßy tinh ho·∫∑c n·ªìi inox c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë m·∫πo ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc hi·ªáu qu·∫£:\n\n1. **Ch·ªçn n·ªìi ph√π h·ª£p**: N·ªìi th·ªßy tinh ho·∫∑c n·ªìi inox c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p s·∫Ω gi√∫p n∆∞·ªõc s√¥i nhanh h∆°n v√† ti·∫øt ki·ªám nƒÉng l∆∞·ª£ng.\n2. **S·ª≠ d·ª•ng b·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas**: B·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas s·∫Ω gi√∫p ƒëun s√¥i n∆∞·ªõc nhanh h∆°n v√† ti·∫øt ki·ªám nƒÉng l∆∞·ª£ng h∆°n so v·ªõi b·∫øp than ho·∫∑c b·∫øp c·ªßi.\n3. **ƒêun n∆∞·ªõc ·ªü nhi·ªát ƒë·ªô v·ª´a ph·∫£i**: ƒêun n∆∞·ªõc ·ªü nhi·ªát ƒë·ªô v·ª´a ph·∫£i (kho·∫£ng 80-90¬∞C) s·∫Ω gi√∫p n∆∞·ªõc s√¥i nhanh h∆°n v√† ti·∫øt ki·ªám nƒÉng l∆∞·ª£ng h∆°n.\n4. **Tr√°nh ƒëun n∆∞·ªõc qu√° n√≥ng**: Tr√°nh ƒëun n∆∞·ªõc qu√° n√≥ng, v√¨ ƒëi·ªÅu n√†y c√≥ th·ªÉ l√†m n∆∞·ªõc m·∫•t ƒëi c√°c ch·∫•t dinh d∆∞·ª°ng v√† t·∫°o ra kh√≠ ƒë·ªôc.\n\n**T·∫°i sao b·∫ßu tr·ªùi l·∫°i c√≥ m√†u xanh**\n\nB·∫ßu tr·ªùi c√≥ m√†u xanh do s·ª± t∆∞∆°ng t√°c gi·ªØa √°nh s√°ng M·∫∑t tr·ªùi v√† c√°c ph√¢n t·ª≠ kh√≠ trong kh√≠ quy·ªÉn Tr√°i ƒê·∫•t. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë l√Ω do t·∫°i sao b·∫ßu tr·ªùi l·∫°i c√≥ m√†u xanh:\n\n1. **T∆∞∆°ng t√°c gi·ªØa √°nh s√°ng v√† kh√≠ quy·ªÉn**: Khi √°nh s√°ng M·∫∑t tr·ªùi chi·∫øu xu·ªëng Tr√°i ƒê·∫•t, n√≥ s·∫Ω t∆∞∆°ng t√°c v·ªõi c√°c ph√¢n t·ª≠ kh√≠ trong kh√≠ quy·ªÉn, bao g·ªìm c·∫£ oxy v√† nit∆°.\n2. **Ph√¢n t√°n √°nh s√°ng**: C√°c ph√¢n t·ª≠ kh√≠ trong kh√≠ quy·ªÉn s·∫Ω ph√¢n t√°n √°nh s√°ng M·∫∑t tr·ªùi, t·∫°o ra m·ªôt ph·ªï √°nh s√°ng r·ªông.\n3. **M√†u xanh l√† m√†u ph·ªï bi·∫øn nh·∫•t**: M√†u xanh l√† m√†u ph·ªï bi·∫øn nh·∫•t trong ph·ªï √°nh s√°ng, v√¨ n√≥ c√≥ b∆∞·ªõc s√≥ng ng·∫Øn nh·∫•t (kho·∫£ng 450-495 nanomet). Do ƒë√≥, m√†u xanh s·∫Ω ƒë∆∞·ª£c nh√¨n th·∫•y nhi·ªÅu nh·∫•t khi √°nh s√°ng M·∫∑t tr·ªùi chi·∫øu xu·ªëng Tr√°i ƒê·∫•t.\n4. **T√°c ƒë·ªông c·ªßa c√°c y·∫øu t·ªë kh√°c**: C√°c y·∫øu t·ªë kh√°c nh∆∞ th·ªùi ti·∫øt, ƒë·ªô cao v√† ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ c≈©ng c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn m√†u s·∫Øc c·ªßa b·∫ßu tr·ªùi.\n\nT√≥m l·∫°i, m√†u xanh c·ªßa b·∫ßu tr·ªùi l√† k·∫øt qu·∫£ c·ªßa s·ª± t∆∞∆°ng t√°c gi·ªØa √°nh s√°ng M·∫∑t tr·ªùi v√† c√°c ph√¢n t·ª≠ kh√≠ trong kh√≠ quy·ªÉn Tr√°i ƒê·∫•t."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"Ph∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc l√† g√¨? V√† t·∫°i sao b·∫ßu tr·ªùi l·∫°i c√≥ m√†u xanh?\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejzahazBZ8vk"
      },
      "source": [
        "‚úÖ Recommended. The prompts below asks one task a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C5ckp2F0Z_Ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "1a802372-8f93-489c-d772-e1c31769d59f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** Ph∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc l√† s·ª≠ d·ª•ng b·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas v·ªõi ch·∫ø ƒë·ªô ƒëun n√≥ng ƒë·ªÅu v√† ·ªïn ƒë·ªãnh. ƒêi·ªÅu n√†y gi√∫p n∆∞·ªõc ƒë∆∞·ª£c ƒëun n√≥ng nhanh ch√≥ng v√† ƒë·ªÅu, gi·∫£m thi·ªÉu nguy c∆° n∆∞·ªõc b·ªã qu√° nhi·ªát ho·∫∑c kh√¥ng ƒë·∫°t ƒë·∫øn nhi·ªát ƒë·ªô s√¥i.\n\nN·∫øu kh√¥ng c√≥ b·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas, b·∫°n c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng b·∫øp than ho·∫∑c b·∫øp c·ªßi, nh∆∞ng c·∫ßn ph·∫£i ƒëi·ªÅu ch·ªânh nhi·ªát ƒë·ªô v√† th·ªùi gian ƒëun n√≥ng ph√π h·ª£p ƒë·ªÉ tr√°nh n∆∞·ªõc b·ªã qu√° nhi·ªát.\n\nM·ªôt s·ªë m·∫πo ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc hi·ªáu qu·∫£:\n\n- S·ª≠ d·ª•ng n·ªìi c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p v·ªõi l∆∞·ª£ng n∆∞·ªõc c·∫ßn ƒëun.\n- ƒêun n∆∞·ªõc ·ªü ch·∫ø ƒë·ªô th·∫•p ban ƒë·∫ßu v√† tƒÉng d·∫ßn nhi·ªát ƒë·ªô khi n∆∞·ªõc b·∫Øt ƒë·∫ßu s√¥i.\n- Kh√¥ng ƒë·ªÉ n∆∞·ªõc qu√° g·∫ßn ng·ªçn l·ª≠a ho·∫∑c ngu·ªìn nhi·ªát ƒë·ªÉ tr√°nh n∆∞·ªõc b·ªã qu√° nhi·ªát.\n- S·ª≠ d·ª•ng d·ª•ng c·ª• ƒëo nhi·ªát ƒë·ªô ƒë·ªÉ ki·ªÉm tra nhi·ªát ƒë·ªô n∆∞·ªõc v√† ƒë·∫£m b·∫£o n∆∞·ªõc ƒë√£ ƒë·∫°t ƒë·∫øn nhi·ªát ƒë·ªô s√¥i.\n\nT√≥m l·∫°i, ph∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc l√† s·ª≠ d·ª•ng b·∫øp ƒëi·ªán ho·∫∑c b·∫øp gas v·ªõi ch·∫ø ƒë·ªô ƒëun n√≥ng ƒë·ªÅu v√† ·ªïn ƒë·ªãnh, v√† tu√¢n th·ªß c√°c m·∫πo tr√™n ƒë·ªÉ ƒë·∫£m b·∫£o n∆∞·ªõc ƒë∆∞·ª£c ƒëun n√≥ng hi·ªáu qu·∫£ v√† an to√†n."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"Ph∆∞∆°ng ph√°p t·ªët nh·∫•t ƒë·ªÉ ƒëun s√¥i n∆∞·ªõc l√† g√¨?\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KwUzhud4aA89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "f9a73e65-6148-4ea1-820f-78edb45dc04f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** T√¥i kh√¥ng c√≥ th√¥ng tin c·ª• th·ªÉ v·ªÅ Ho√†ng, v√¨ v·∫≠y t√¥i kh√¥ng th·ªÉ ƒë√°nh gi√° ƒë∆∞·ª£c v·∫ª ƒë·∫πp trai c·ªßa anh ·∫•y. N·∫øu b·∫°n ƒëang h·ªèi v·ªÅ m·ªôt ng∆∞·ªùi c·ª• th·ªÉ, c√≥ th·ªÉ b·∫°n c·∫ßn cung c·∫•p th√™m th√¥ng tin v·ªÅ Ho√†ng ƒë·ªÉ t√¥i c√≥ th·ªÉ gi√∫p b·∫°n t·ªët h∆°n."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"T·∫°i sao b·∫ßu tr·ªùi l·∫°i c√≥ m√†u xanh?\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJIL2RTQaGcT"
      },
      "source": [
        "### Watch out for hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y8kYxrSaHE9"
      },
      "source": [
        "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isn't a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.\n",
        "\n",
        "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NY5nAGeaJYS"
      },
      "source": [
        "Try the prompt like the one below repeatedly. We set the temperature to `1.0` so that it takes more risks in its choices. It's possible that it may provide an inaccurate, but confident answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QALPjEILaM62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "60dbf480-604d-4a06-9cf7-525c5e8758bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** ƒê·ªÉ bi·∫øt ng√†y h√¥m nay l√† ng√†y m·∫•y, t√¥i c·∫ßn bi·∫øt ng√†y hi·ªán t·∫°i. Tuy nhi√™n, t√¥i kh√¥ng th·ªÉ bi·∫øt ch√≠nh x√°c ng√†y h√¥m nay v√¨ th√¥ng tin c·ªßa t√¥i ch·ªâ ƒë·∫øn nƒÉm 2023.\n\nN·∫øu b·∫°n mu·ªën bi·∫øt ng√†y h√¥m nay l√† ng√†y m·∫•y, h√£y cho t√¥i bi·∫øt ng√†y v√† th√°ng c·ªßa nƒÉm 2024 ho·∫∑c th√¥ng tin kh√°c ƒë·ªÉ t√¥i c√≥ th·ªÉ gi√∫p b·∫°n."
          },
          "metadata": {}
        }
      ],
      "source": [
        "llm_1 = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=MODEL_ID,\n",
        "    temperature=1.0,\n",
        "    max_tokens=256,\n",
        "    #reasoning_format=\"raw\"\n",
        ")\n",
        "\n",
        "# Prompt b·∫±ng ti·∫øng Vi·ªát\n",
        "prompt_vi = \"H√¥m nay l√† ng√†y m·∫•y?\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm_1.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRkwzbgRbhKt"
      },
      "source": [
        "Since LLMs do not have access to real-time information without further integrations, you may have noticed it hallucinates what day it is today in some of the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c811e310d02"
      },
      "source": [
        "### Using system instructions to guardrail the model from irrelevant responses\n",
        "\n",
        "How can we attempt to reduce the chances of irrelevant responses and hallucinations?\n",
        "\n",
        "One way is to provide the LLM with [system instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction).\n",
        "\n",
        "Let's see how system instructions works and how you can use them to reduce hallucinations or irrelevant questions for a travel chatbot.\n",
        "\n",
        "Suppose we ask a simple question about one of Italy's most famous tourist spots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rB6zJU76biFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "98762829-424a-49fe-8c8c-cd493b5a3568"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** Milan l√† m·ªôt trong nh·ªØng th√†nh ph·ªë l·ªõn v√† phong ph√∫ nh·∫•t c·ªßa √ù, v·ªõi nhi·ªÅu ƒë·ªãa ƒëi·ªÉm tham quan h·∫•p d·∫´n. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë ƒë·ªãa ƒëi·ªÉm tham quan t·ªët nh·∫•t ·ªü Milan:\n\n1. **ƒê√†i th√°p Duomo**: ƒê√¢y l√† m·ªôt trong nh·ªØng bi·ªÉu t∆∞·ª£ng n·ªïi ti·∫øng nh·∫•t c·ªßa Milan, ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ nƒÉm 1386. ƒê√†i th√°p Duomo l√† m·ªôt c√¥ng tr√¨nh ki·∫øn tr√∫c Gothic l·ªõn v·ªõi 3.400 m·∫£nh ƒë√° granite.\n\n2. **S√¢n v·∫≠n ƒë·ªông San Siro**: ƒê√¢y l√† s√¢n v·∫≠n ƒë·ªông b√≥ng ƒë√° l·ªõn nh·∫•t c·ªßa Milan, c≈©ng l√† s√¢n nh√† c·ªßa hai ƒë·ªôi b√≥ng ƒë√° ch√≠nh c·ªßa th√†nh ph·ªë: AC Milan v√† Inter Milan.\n\n3. **B·∫£o t√†ng Pinacoteca di Brera**: ƒê√¢y l√† m·ªôt trong nh·ªØng b·∫£o t√†ng ngh·ªá thu·∫≠t l·ªõn nh·∫•t c·ªßa √ù, v·ªõi h∆°n 40.000 t√°c ph·∫©m ngh·ªá thu·∫≠t t·ª´ th·ªùi Trung c·ªï ƒë·∫øn hi·ªán ƒë·∫°i.\n\n4. **Nh√† th·ªù La Scala**: ƒê√¢y l√† m·ªôt trong nh·ªØng nh√† h√°t Opera l·ªõn nh·∫•t th·∫ø gi·ªõi, ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ nƒÉm 1776. Nh√† h√°t n√†y ƒë√£ t·ª´ng t·ªï ch·ª©c nhi·ªÅu bu·ªïi bi·ªÉu di·ªÖn opera n·ªïi ti·∫øng c·ªßa c√°c nh√† so·∫°n nh·∫°c nh∆∞ Verdi v√† Mozart.\n\n5. **C·∫ßu Ponte Vecchio**: ƒê√¢y l√†"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "llm_2 = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=MODEL_ID,\n",
        "    temperature=1.0,\n",
        "    max_tokens=256,\n",
        "    #reasoning_format=\"raw\",\n",
        ")\n",
        "\n",
        "# 2. Chu·∫©n b·ªã n·ªôi dung System\n",
        "sys_instruction_text = \"B·∫°n l√† chatbot du l·ªãch. Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ du l·ªãch.\"\n",
        "\n",
        "# Prompt b·∫±ng ti·∫øng Vi·ªát\n",
        "prompt_vi = \"ƒê·ªãa ƒëi·ªÉm tham quan t·ªët nh·∫•t ·ªü Milan, √ù l√† ƒë√¢u?\"\n",
        "\n",
        "# 3. T·∫°o danh s√°ch tin nh·∫Øn ƒë·ªÉ g·ª≠i ƒëi\n",
        "messages = [\n",
        "    SystemMessage(content=sys_instruction_text), # Set vai tr√≤ ·ªü ƒë√¢y\n",
        "    HumanMessage(content=prompt_vi)\n",
        "]\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm_2.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZa-Qcf9cF4A"
      },
      "source": [
        "Now let us pretend to be a user asks the chatbot a question that is unrelated to travel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AZKBIDr2cGnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "b205d20b-bfdd-4ced-da18-8aeb561c7706"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** ƒê·ªÉ l√†m b√°nh pizza ngon v√† ngon mi·ªáng, b·∫°n c·∫ßn th·ª±c hi·ªán c√°c b∆∞·ªõc sau:\n\n**Nguy√™n li·ªáu:**\n\n- 1 l√≠t n∆∞·ªõc\n- 1 th√¨a ƒë∆∞·ªùng\n- 2 th√¨a mu·ªëi\n- 3 th√¨a d·∫ßu th·ª±c v·∫≠t\n- 4 cup b·ªôt m√¨ tr·∫Øng\n- 1 th√¨a b·ªôt n·ªü\n- 1/2 th√¨a mu·ªëi\n- B·ªôt m√¨ d·∫°ng b·ªôt m√¨ ƒëa d·ª•ng\n- N∆∞·ªõc √©p c√† chua (ho·∫∑c n∆∞·ªõc s·ªët c√† chua)\n- S·ªØa ho·∫∑c d·∫ßu th·ª±c v·∫≠t\n- Mu·ªëi v√† ƒë∆∞·ªùng (t√πy ch·ªçn)\n- C√°c th√†nh ph·∫ßn toppings (nh∆∞ ph√¥ mai, th·ªãt x√¥ng kh√≥i, √¥-veau, h√†nh, v√† c√°c lo·∫°i rau kh√°c)\n\n**C√°ch l√†m:**\n\n1. **Kh·ªüi ƒë·ªông**: tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu l√†m b√°nh pizza, b·∫°n n√™n chu·∫©n b·ªã b·ªÅ m·∫∑t l√†m vi·ªác s·∫°ch s·∫Ω v√† tho√°ng m√°t, ƒë·ªìng th·ªùi ki·ªÉm tra l·∫°i danh s√°ch nguy√™n li·ªáu v√† ƒë·∫£m b·∫£o r·∫±ng b·∫°n c√≥ ƒë·ªß nguy√™n li·ªáu.\n\n2. **Tr·ªôn b·ªôt**: Tr·ªôn b·ªôt m√¨, b·ªôt n·ªü, mu·ªëi v√† ƒë∆∞·ªùng trong m·ªôt chi·∫øc b√°t l·ªõn. Th√™m v√†o 3 cups n∆∞·ªõc v√† khu·∫•y ƒë·ªÅu cho ƒë·∫øn khi b·ªôt m·ªãn v√† kh√¥ng c√≤n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"L√†m sao ƒë·ªÉ l√†m b√°nh Pizza\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm_2.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiUYIhwpctCy"
      },
      "source": [
        "You can see that this way, a guardrail in the prompt prevented the chatbot from veering off course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xASHAkc46n"
      },
      "source": [
        "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nPfXQWIacwRf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "6f092e34-44e7-4858-a04b-405f82799791"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** T√¥i c√≥ m·ªôt s·ªë g·ª£i √Ω ho·∫°t ƒë·ªông l·∫≠p tr√¨nh th√∫ v·ªã ƒë·ªÉ gi√∫p b·∫°n c·∫£i thi·ªán k·ªπ nƒÉng l·∫≠p tr√¨nh:\n\n1. **T·∫°o tr√≤ ch∆°i ƒë∆°n gi·∫£n**: B·∫°n c√≥ th·ªÉ t·∫°o m·ªôt tr√≤ ch∆°i ƒë∆°n gi·∫£n nh∆∞ Tic Tac Toe, Hangman ho·∫∑c Snake b·∫±ng ng√¥n ng·ªØ l·∫≠p tr√¨nh c·ªßa m√¨nh (v√≠ d·ª•: Python, JavaScript, C++). ƒêi·ªÅu n√†y s·∫Ω gi√∫p b·∫°n √°p d·ª•ng ki·∫øn th·ª©c v·ªÅ logic, bi·∫øn v√† ƒëi·ªÅu khi·ªÉn lu·ªìng.\n2. **T·∫°o ·ª©ng d·ª•ng t√≠nh to√°n**: B·∫°n c√≥ th·ªÉ t·∫°o m·ªôt ·ª©ng d·ª•ng t√≠nh to√°n ƒë∆°n gi·∫£n nh∆∞ m√°y t√≠nh b·∫£ng, ·ª©ng d·ª•ng t√≠nh to√°n ti·ªÅn t·ªá ho·∫∑c ·ª©ng d·ª•ng t√≠nh to√°n th·ªùi ti·∫øt. ƒêi·ªÅu n√†y s·∫Ω gi√∫p b·∫°n √°p d·ª•ng ki·∫øn th·ª©c v·ªÅ bi·∫øn, to√°n h·ªçc v√† nh·∫≠p/output.\n3. **T·∫°o ·ª©ng d·ª•ng qu·∫£n l√Ω danh s√°ch**: B·∫°n c√≥ th·ªÉ t·∫°o m·ªôt ·ª©ng d·ª•ng qu·∫£n l√Ω danh s√°ch ƒë∆°n gi·∫£n nh∆∞ ·ª©ng d·ª•ng qu·∫£n l√Ω danh s√°ch mua s·∫Øm, ·ª©ng d·ª•ng qu·∫£n l√Ω danh s√°ch c√¥ng vi·ªác ho·∫∑c ·ª©ng d·ª•ng qu·∫£n l√Ω danh s√°ch h·ªçc t·∫≠p. ƒêi·ªÅu n√†y s·∫Ω gi√∫p b·∫°n √°p d·ª•ng ki·∫øn th·ª©c v·ªÅ m·∫£ng, v√≤ng l·∫∑p v√† nh·∫≠p/output.\n4. **T·∫°o ·ª©ng d·ª•ng ƒë·ªì h·ªça**: B·∫°n c√≥ th·ªÉ t·∫°o m·ªôt ·ª©ng d·ª•ng ƒë·ªì h·ªça ƒë∆°n gi·∫£n nh∆∞ ·ª©ng d·ª•ng t·∫°o h√¨nh ·∫£nh, ·ª©ng d·ª•ng t·∫°o video ho·∫∑c ·ª©ng d·ª•ng"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"T√¥i l√† h·ªçc sinh trung h·ªçc. H√£y g·ª£i √Ω m·ªôt ho·∫°t ƒë·ªông l·∫≠p tr√¨nh ƒë·ªÉ c·∫£i thi·ªán k·ªπ nƒÉng c·ªßa t√¥i.\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAmm9wPYc_1o"
      },
      "source": [
        "## Classification tasks reduces output variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvRpK_0GdCpf"
      },
      "source": [
        "The prompt below results in a choice and may be useful if you want the output to be easier to control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kYDKh0r2dAqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "659b0c88-611c-4692-d388-a0ad4d9c6c0f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** T√¥i khuy√™n b·∫°n n√™n ch·ªçn ho·∫°t ƒë·ªông \"a) h·ªçc Python\". L√Ω do l√†:\n\n- Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh ph·ªï bi·∫øn v√† ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c nh∆∞ khoa h·ªçc m√°y t√≠nh, d·ªØ li·ªáu, m√°y h·ªçc, v√† t·ª± ƒë·ªông h√≥a.\n- V·ªõi Python, b·∫°n c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c c√°c k·ªπ nƒÉng l·∫≠p tr√¨nh c∆° b·∫£n v√† n√¢ng cao, ƒë·ªìng th·ªùi c√≥ th·ªÉ x√¢y d·ª±ng c√°c d·ª± √°n th·ª±c t·∫ø trong nhi·ªÅu lƒ©nh v·ª±c.\n- Python c≈©ng r·∫•t d·ªÖ h·ªçc v√† c√≥ c·ªông ƒë·ªìng ph√°t tri·ªÉn l·ªõn, n√™n b·∫°n c√≥ th·ªÉ t√¨m th·∫•y nhi·ªÅu t√†i nguy√™n h·ªçc t·∫≠p v√† h·ªó tr·ª£ tr·ª±c tuy·∫øn.\n- Ngo√†i ra, vi·ªác h·ªçc Python c√≥ th·ªÉ gi√∫p b·∫°n ph√°t tri·ªÉn k·ªπ nƒÉng t∆∞ duy logic, gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ, v√† t·∫°o ra c√°c gi·∫£i ph√°p s√°ng t·∫°o.\n\nHo·∫°t ƒë·ªông \"b) h·ªçc JavaScript\" c≈©ng r·∫•t th√∫ v·ªã, nh∆∞ng n√≥ th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c d·ª± √°n web v√† ·ª©ng d·ª•ng di ƒë·ªông, n√™n c√≥ th·ªÉ h∆°i kh√°c so v·ªõi nhu c·∫ßu c·ªßa m·ªôt h·ªçc sinh trung h·ªçc.\n\nHo·∫°t ƒë·ªông \"c) Ng·ªß\" l√† c·∫ßn thi·∫øt ƒë·ªÉ ph·ª•c h·ªìi s·ª©c kh·ªèe v√† tr√≠ n√£o sau m·ªôt ng√†y h·ªçc t·∫≠p, nh∆∞ng kh√¥ng ph·∫£i l√† ho·∫°t ƒë·ªông t·∫°o ra gi√° tr·ªã ho·∫∑c k·ªπ nƒÉng m·ªõi."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt_vi = \"\"\"T√¥i l√† h·ªçc sinh trung h·ªçc. B·∫°n g·ª£i √Ω ho·∫°t ƒë·ªông n√†o trong c√°c ho·∫°t ƒë·ªông d∆∞·ªõi ƒë√¢y v√† gi·∫£i th√≠ch l√Ω do:\n",
        "a) h·ªçc Python\n",
        "b) h·ªçc JavaScript\n",
        "c) Ng·ªß\n",
        "\"\"\"\n",
        "\n",
        "# G·ªçi LLM\n",
        "response_vi = llm.invoke(prompt_vi)\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "display(Markdown(f\"**Output:** {response_vi.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTd60b1GdIsx"
      },
      "source": [
        "## Improve response quality by including examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJi44NejdJYE"
      },
      "source": [
        "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
        "\n",
        "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMbLginWdOKs"
      },
      "source": [
        "#### Zero-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crh2Loi2dQ0v"
      },
      "source": [
        "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-7myRc-SdTQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "19a93e71-af1b-4648-9106-36cebfeca0ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** C·∫£m x√∫c c·ªßa c√¢u n√≥i tr√™n l√†: T√≠ch c·ª±c.\n\nC√¢u n√≥i n√†y th·ªÉ hi·ªán s·ª± ng∆∞·ª°ng m·ªô, khen ng·ª£i v·ªÅ v·∫ª ƒë·∫πp c·ªßa ng∆∞·ªùi ƒë∆∞·ª£c n√≥i ƒë·∫øn, t·∫°o ra c·∫£m gi√°c t√≠ch c·ª±c v√† t√≠ch c·ª±c."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "#from langchain import LLMChain, PromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model=MODEL_ID,\n",
        "    temperature=0.7,  # V√≠ d·ª•: 0.7\n",
        "    max_tokens=256,    # V√≠ d·ª•: 256\n",
        "    #reasoning_format=\"raw\"\n",
        ")\n",
        "\n",
        "template = \"\"\"\n",
        "X√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa c√¢u n√≥i sau: t√≠ch c·ª±c, trung l·∫≠p hay ti√™u c·ª±c.\n",
        "\n",
        "Tweet: {tweet_text}\n",
        "C·∫£m x√∫c:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"tweet_text\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "#chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "tweet_example = \"Anh Nh√¢n ƒë·∫πp trai qu√°!\"\n",
        "\n",
        "response = chain.invoke({\"tweet_text\": tweet_example})\n",
        "\n",
        "display(Markdown(f\"**Output:** {response.content}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucRtPn9SdL64"
      },
      "source": [
        "#### One-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs0gQH2vdYBi"
      },
      "source": [
        "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "iEq-KxGYdaT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "99ba1eca-2345-4c72-fc21-83c905595202"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** ƒê·ªÉ x√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa Tweet, ch√∫ng ta c·∫ßn ph√¢n t√≠ch n·ªôi dung v√† ng·ªØ ƒëi·ªáu c·ªßa Tweet ƒë√≥.\n\nV√≠ d·ª•, Tweet: Ho√†ng h·ªçc gi·ªèi vcl\n- N·ªôi dung c·ªßa Tweet n√≥i v·ªÅ vi·ªác Ho√†ng h·ªçc gi·ªèi\n- Ng·ªØ ƒëi·ªáu c·ªßa Tweet kh√¥ng c√≥ d·∫•u hi·ªáu ti√™u c·ª±c\n- Do ƒë√≥, c·∫£m x√∫c c·ªßa Tweet l√†: t√≠ch c·ª±c\n\nC·∫£m x√∫c c·ªßa Tweet: t√≠ch c·ª±c\nSentiment: t√≠ch c·ª±c"
          },
          "metadata": {}
        }
      ],
      "source": [
        "template = \"\"\"\n",
        "X√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa Tweet: t√≠ch c·ª±c, trung l·∫≠p hay ti√™u c·ª±c.\n",
        "\n",
        "V√≠ d·ª•:\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: {tweet_text}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"tweet_text\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "#chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "tweet_example = \"Ho√†ng h·ªçc gi·ªèi vcl\"\n",
        "\n",
        "response = chain.invoke({\"tweet_text\": tweet_example})\n",
        "\n",
        "display(Markdown(f\"**Output:** {response.content}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnKLjJzmdfL_"
      },
      "source": [
        "#### Few-shot prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zv-9F5OdgI_"
      },
      "source": [
        "Below is an example of few-shot prompting, where you provide a few examples to the LLM within the prompt to give some guidance on what type of response you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "u37P9tG4dk9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "f9cdb0c6-7e7e-4761-8589-c5ab0f5ca11d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Output:** ƒê·ªÉ x√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa m·ªôt Tweet, ta c·∫ßn ph√¢n t√≠ch n·ªôi dung c·ªßa tweet ƒë√≥. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch cho v√≠ d·ª• c·ªßa b·∫°n:\n\nTweet: Anh Nh√¢n x·∫•u trai v√† Ho√†ng ƒë·∫πp trai\nN·ªôi dung c·ªßa tweet n√†y c√≥ m·ªôt √Ω ki·∫øn ti√™u c·ª±c v·ªÅ ng∆∞·ªùi t√™n Anh Nh√¢n (\"x·∫•u trai\") v√† m·ªôt √Ω ki·∫øn t√≠ch c·ª±c v·ªÅ ng∆∞·ªùi t√™n Ho√†ng (\"ƒë·∫πp trai\"). V√¨ c√≥ c·∫£ hai √Ω ki·∫øn t√≠ch c·ª±c v√† ti√™u c·ª±c n√™n sentiment c·ªßa tweet n√†y kh√¥ng ph·∫£i l√† t√≠ch c·ª±c hay ti√™u c·ª±c tuy·ªát ƒë·ªëi, nh∆∞ng c√≥ xu h∆∞·ªõng ti√™u c·ª±c h∆°n.\n\nSuy ra, Sentiment: ti√™u c·ª±c\n\nTuy nhi√™n, c√°ch ph√¢n t√≠ch n√†y c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100% n·∫øu kh√¥ng c√≥ ng·ªØ c·∫£nh r√µ r√†ng v√¨ c√≥ th·ªÉ tweet n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng v·ªõi m·ª•c ƒë√≠ch nh·∫°o b√°ng ho·∫∑c kh√¥ng nghi√™m t√∫c."
          },
          "metadata": {}
        }
      ],
      "source": [
        "template = \"\"\"\n",
        "X√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa Tweet: t√≠ch c·ª±c, trung l·∫≠p hay ti√™u c·ª±c.\n",
        "\n",
        "V√≠ d·ª•:\n",
        "Tweet: I loved the new YouTube video you made!\n",
        "Sentiment: positive\n",
        "\n",
        "Tweet: That was awful. Super boring üò†\n",
        "Sentiment: negative\n",
        "\n",
        "Tweet: {tweet_text}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"tweet_text\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "#chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "tweet_example = \"Anh Nh√¢n x·∫•u trai v√† Ho√†ng ƒë·∫πp trai\"\n",
        "\n",
        "response = chain.invoke({\"tweet_text\": tweet_example})\n",
        "\n",
        "display(Markdown(f\"**Output:** {response.content}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}