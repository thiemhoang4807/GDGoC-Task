{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYZeYOuyJ-ws",
        "outputId": "da910ad7-9851-40f3-f62f-5c8f4cd58910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.12/dist-packages (8.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from underthesea) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from underthesea) (6.0.3)\n",
            "Requirement already satisfied: underthesea_core==1.0.5 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.0.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnQgdYMHDdSj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from underthesea import word_tokenize as vn_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "dataset = [\n",
        "    \"Học NLP thật thú vị!\",\n",
        "    \"Natural Language Processing is fun.\",\n",
        "    \"Tôi không thích món ăn này, nó quá mặn.\",\n",
        "    \"Trường UIT là ngôi trường danh tiếng.\",\n",
        "    \"Don't panic! It's going to be fine.\"\n",
        "]"
      ],
      "metadata": {
        "id": "welSjzBNJx6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tách bằng khoảng trắng"
      ],
      "metadata": {
        "id": "B7Nhaw23N7o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Tokenization bằng phương pháp tách bằng khoảng trắng. ---\")\n",
        "for sentence in dataset:\n",
        "    tokens = sentence.split()\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Tokens  : {tokens}\\n\")\n",
        "\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaz8FeaTJ6jO",
        "outputId": "20aa8d2d-6331-45bf-fc30-bc40e83f954b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tokenization bằng phương pháp tách bằng khoảng trắng. ---\n",
            "Original: Học NLP thật thú vị!\n",
            "Tokens  : ['Học', 'NLP', 'thật', 'thú', 'vị!']\n",
            "\n",
            "Original: Natural Language Processing is fun.\n",
            "Tokens  : ['Natural', 'Language', 'Processing', 'is', 'fun.']\n",
            "\n",
            "Original: Tôi không thích món ăn này, nó quá mặn.\n",
            "Tokens  : ['Tôi', 'không', 'thích', 'món', 'ăn', 'này,', 'nó', 'quá', 'mặn.']\n",
            "\n",
            "Original: Trường UIT là ngôi trường danh tiếng.\n",
            "Tokens  : ['Trường', 'UIT', 'là', 'ngôi', 'trường', 'danh', 'tiếng.']\n",
            "\n",
            "Original: Don't panic! It's going to be fine.\n",
            "Tokens  : [\"Don't\", 'panic!', \"It's\", 'going', 'to', 'be', 'fine.']\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tách bằng NLTK"
      ],
      "metadata": {
        "id": "oLZr51hUN_U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 2. NLTK TOKENIZATION ---\")\n",
        "eng = [1, 4]\n",
        "for i in eng:\n",
        "    sentence = dataset[i]\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Tokens  : {tokens}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ1yGMw-KPxq",
        "outputId": "bb3aaf72-7371-4c17-9e87-3839a2e0e6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. NLTK TOKENIZATION ---\n",
            "Original: Natural Language Processing is fun.\n",
            "Tokens  : ['Natural', 'Language', 'Processing', 'is', 'fun', '.']\n",
            "\n",
            "Original: Don't panic! It's going to be fine.\n",
            "Tokens  : ['Do', \"n't\", 'panic', '!', 'It', \"'s\", 'going', 'to', 'be', 'fine', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tách bằng Undersea"
      ],
      "metadata": {
        "id": "KIbf2vx0ODHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 3. UNDERTHESEA TOKENIZATION ---\")\n",
        "vnm = [0, 2, 3]\n",
        "for i in vnm:\n",
        "    sentence = dataset[i]\n",
        "    tokens = vn_tokenize(sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Tokens  : {tokens}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfMrPBKDKZe6",
        "outputId": "d1f7adc4-ef83-47cf-df0d-ef91c2d767b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 3. UNDERTHESEA TOKENIZATION ---\n",
            "Original: Học NLP thật thú vị!\n",
            "Tokens  : ['Học NLP', 'thật', 'thú vị', '!']\n",
            "\n",
            "Original: Tôi không thích món ăn này, nó quá mặn.\n",
            "Tokens  : ['Tôi', 'không', 'thích', 'món', 'ăn', 'này', ',', 'nó', 'quá', 'mặn', '.']\n",
            "\n",
            "Original: Trường UIT là ngôi trường danh tiếng.\n",
            "Tokens  : ['Trường', 'UIT', 'là', 'ngôi', 'trường', 'danh tiếng', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trả lời câu hỏi và phân tích\n",
        "##Câu 1.\n",
        "###1. Tách bằng khoảng trắng\n",
        "- Dấu cuối câu dính vào chữ cuối cùng.\n",
        "- Không tách được từ viết tắt (Don't)\n",
        "###2. Sử dụng thư viện\n",
        "- NLTK: Xử lí thông minh các quy tắc ngữ pháp tiếng anh (tách được các từ viết tắt), tách riêng dấu câu\n",
        "- Undersea: Có thể nhận diện các từ ghép (thú vị, món ăn, danh tiếng). Trong tiếng Việt, nghĩa của các từ thường nằm ở các cụm từ ghép, không phải riêng mỗi âm tiết.\n",
        "##Câu 2.\n",
        "Sự mơ hồ là khi một đầu vào ngôn ngữ có thể được hiểu theo nhiều cách khác nhau, và máy tính (vốn làm việc theo logic 0-1) rất khó để chọn ra ý nghĩa đúng nếu thiếu ngữ cảnh hoặc kiến thức nền.\n",
        "### Từ đồng âm khác nghĩa\n",
        "Ví dụ: từ \"đàn\"\n",
        "- Bạn đánh đàn hay thật.\n",
        "- Cây gỗ bạch đàn này đã trăm tuổi rồi.\n",
        "\n",
        "-> Nếu chỉ nhìn vào từ \"đàn\" (sau khi tokenization), máy tính không biết nên gán ngữ nghĩa nào cho nó nếu không xem xét các từ xung quanh.\n",
        "\n",
        "### Cấu trúc câu\n",
        "Ví dụ: Hổ mang bò lên núi.\n",
        "Có 2 cách hiểu:\n",
        "- Cách 1: Con rắn hổ mang đang bò lên núi\n",
        "- Cách 2: Con hổ mang con bò lên núi\n",
        "\n",
        "-> Việc máy tính tách từ sai ở bước này sẽ dẫn đến việc hiểu sai hoàn toàn chủ ngữ và hành động của câu.\n",
        "\n",
        "##Câu 3.\n",
        "Câu input: \"Học sinh học sinh học tại trường\".\n",
        "\n",
        "Nếu dùng Space-based Tokenization:\n",
        "\n",
        "Tokens: ['Học', 'sinh', 'học', 'sinh', 'học', 'tại', 'trường']\n",
        "\n",
        "Vấn đề gặp phải:\n",
        "- Không nhận diện được 2 từ quan trọng là \"học sinh\" với \"sinh học\"\n",
        "- Máy tính có thể hiểu chữ \"sinh\" là sinh đẻ, sinh sống...\n",
        "\n",
        "-> Chatbot có thể trả lời sai."
      ],
      "metadata": {
        "id": "OAbbfoAyNE-X"
      }
    }
  ]
}